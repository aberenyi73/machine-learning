{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness Study\n",
    "This notebook contains the work evaluating robustness of the final model. Result and finding are presented in [capstone%20report.ipynb](capstone%20report.ipynb) file.\n",
    "\n",
    "## Data Preprocessing\n",
    "Read in the data file and call preprocess to encode and remove outliers. Display top 10 rows as sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import table\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from preprocess_visuals import *\n",
    "\n",
    "pd.options.display.max_rows = 160\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/LoanStats_securev1_2017Q1.csv.zip', skiprows=1, skipfooter=2,\n",
    "                 engine='python', usecols=get_usecols(), converters=get_conv())\n",
    "\n",
    "# dummy encode categorical variables and impute missing values\n",
    "df = preprocess(df)\n",
    "df = remove_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Seed\n",
    "Run the final model with various random states in the train-test split and analyze the mean / variance of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tries: None\n",
      "Precision mean: 0.072, std: 0.010 \n",
      "Recall mean: 0.716, std: 0.117 \n",
      "Accuracy mean: 0.570, std: 0.109 \n",
      "F2 score mean: 0.251, std: 0.011 \n",
      "\n",
      "recalls: [0.4618086040386304, 0.8361233480176211, 0.7918436703483432, 0.7409326424870466, 0.8248730964467005, 0.8307291666666666, 0.7636518771331058, 0.4804421768707483, 0.7602389078498294, 0.8409286328460877, 0.7879558948261238, 0.7523809523809524, 0.8344709897610921, 0.8406445837063563, 0.8063139931740614, 0.7648514851485149, 0.6466552315608919, 0.5849870578084556, 0.49828178694158076, 0.719626168224299, 0.7790697674418605, 0.620353982300885, 0.5924686192468619, 0.517566409597258, 0.6418642681929682, 0.7878535773710482, 0.7662447257383966, 0.8464912280701754, 0.6257408975444538, 0.726649528706084]\n"
     ]
    }
   ],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "accuracies = []\n",
    "f2s = []\n",
    "s = 30\n",
    "    \n",
    "for s in np.arange(s):\n",
    "    random.seed(s*3)\n",
    "    ri1 = random.randint(27, 99999)\n",
    "    # Calculate the training and testing scores of best classifier\n",
    "    clf = RandomForestClassifier(criterion='entropy', class_weight='balanced', random_state=ri1, \n",
    "                                 max_depth=1, max_features=50, n_estimators=10)\n",
    "\n",
    "    y = df.loan_status\n",
    "    X = df.drop(columns='loan_status')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=ri1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predicted = clf.predict(X_test)\n",
    "    a = round(accuracy_score(y_test, y_predicted), 4)\n",
    "    p, r, f2, s = precision_recall_fscore_support(y_test, y_predicted, beta=2, average='binary')\n",
    "    \n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "    accuracies.append(a)\n",
    "    f2s.append(f2)\n",
    "\n",
    "print(\"Number of tries:\", s)\n",
    "print(\"Precision mean: %.3f, std: %.3f \" % (np.mean(precisions), np.std(precisions)))\n",
    "print(\"Recall mean: %.3f, std: %.3f \" % (np.mean(recalls), np.std(recalls)))\n",
    "print(\"Accuracy mean: %.3f, std: %.3f \" % (np.mean(accuracies), np.std(accuracies)))\n",
    "print(\"F2 score mean: %.3f, std: %.3f \" % (np.mean(f2s), np.std(f2s)))\n",
    "\n",
    "print(\"\\nrecalls:\", recalls)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic =  9.962 pvalue = 7.1916e-11\n",
      "KS-statistic D =  0.676 pvalue = 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "print('t-statistic = %6.3f pvalue = %6.4e' %  stats.ttest_1samp(recalls, 0.5))\n",
    "\n",
    "# varify that the samples come from a t-distribution using KS-test:\n",
    "print('KS-statistic D = %6.3f pvalue = %6.4f' % stats.kstest(recalls, 't', (30,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results from T-statistic\n",
    "\n",
    "The p-value indicates the probablity that the classifier recall is the same as the naive classifier's recall.\n",
    "Since the sample mean is 7.1916e-11, and it is less than alpha=0.05, we can reject the null hypothesis, and accept the alternative that classifier recall mean is higher.\n",
    "\n",
    "The Kolmogovov-Smirnoff test pvalue < 0.05 indicates that the recall means form a t-distribution with 30 degrees of freedom.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
